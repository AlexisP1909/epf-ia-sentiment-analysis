{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "400fa34d9a714a22a20259ba3140a030551e1986"
   },
   "source": [
    "# Read & Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('./amazonreviews_dataset/7/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./amazonreviews_dataset/7/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df4ce91d1b19fe5c93e401a3ce64fd4d1f07787a"
   },
   "source": [
    "## Create Lists containing Train & Test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "d554f2ba-0eeb-4764-b1df-febfbdda5c2c",
    "_uuid": "93213b70d17edcdeeba304a7344e415733d0ce17"
   },
   "outputs": [],
   "source": [
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4a67bd8e9d198026184e07764feeaa8727288e64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_file, test_file\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0357f1f5175f121b2d2435fd7e2477f4b2a0e17e"
   },
   "source": [
    "## Convert from raw binary strings to strings that can be parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "ceda406d-42f3-46de-930c-f658b1090792",
    "_uuid": "5758abb69d2f0b7434041d5444916089ef468d88"
   },
   "outputs": [],
   "source": [
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "dcc69630-91ef-4a06-9eb4-56c4b0390a49",
    "_uuid": "539afb8a439f11442ad8ca5c1de042a3a310462f"
   },
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "                                                       \n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a1b5fb60ceafd2f0776af45a4722fc55c15903c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_file_lines, test_file_lines\n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
    "_kg_hide-input": true,
    "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Using NLTK Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes\n",
    "Na_train = {'Sentence': train_sentences, 'Label': train_labels}\n",
    "Nav_train = pd.DataFrame(Na_train)\n",
    "\n",
    "Na_test = {'Sentence': test_sentences, 'Label': test_labels}\n",
    "Nav_test = pd.DataFrame(Na_test)\n",
    "\n",
    "Nav_train.head()\n",
    "\n",
    "Nav_train = Nav_train.head(900)\n",
    "Nav_test = Nav_test.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Positive and Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#train_pos = Nav_train[Nav_train['Label'] == 1]\n",
    "#train_pos = Nav_train['Sentence']\n",
    "#train_neg = Nav_train[Nav_train['Label'] == 0]\n",
    "#train_neg = Nav_train['Sentence']\n",
    "\n",
    "test_pos = Nav_test[Nav_test['Label'] == 1]\n",
    "test_pos = Nav_test['Sentence']\n",
    "test_neg = Nav_test[Nav_test['Label'] == 0]\n",
    "test_neg = Nav_test['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del Na_train, Na_test, train_sentences, train_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleaning and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "sents = []\n",
    "alll = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for index, row in Nav_train.iterrows():\n",
    "    words_filtered = [e.lower() for e in row.Sentence.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('@')\n",
    "        and not word.startswith('#')\n",
    "        and word != 'RT']\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    sents.append((words_without_stopwords, row.Label))\n",
    "    alll.extend(words_without_stopwords )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "-# Extracting word features\n",
    "def get_words_in_tweets(tweets):\n",
    "    alll = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        alll.extend(words)\n",
    "    return alll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teee = get_words_in_tweets(sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_features = get_word_features(alll)\n",
    "# TESTING BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Naive Bayes classifier\n",
    "training_set = nltk.classify.apply_features(extract_features,sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = Nav_train[Nav_train['Label'] == 1]\n",
    "train_pos = train_pos['Sentence']\n",
    "train_neg = Nav_train[Nav_train['Label'] == 0]\n",
    "train_neg = train_neg['Sentence']\n",
    "test_pos = Nav_test[Nav_test['Label'] == 1]\n",
    "test_pos = test_pos['Sentence']\n",
    "test_neg = Nav_test[Nav_test['Label'] == 0]\n",
    "test_neg = test_neg['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     batteries died within a year ...: i bought thi...\n",
       "5     dvd player crapped out after one year: i also ...\n",
       "6     incorrect disc: i love the style of this, but ...\n",
       "7     dvd menu select problems: i cannot scroll thro...\n",
       "9     not an \"ultimate guide\": firstly,i enjoyed the...\n",
       "11    not!: if you want to listen to el duke , then ...\n",
       "12    a complete bust: this game requires quicktime ...\n",
       "14    didn't run off of usb bus power: was hoping th...\n",
       "15    don't buy!: first of all, the company took my ...\n",
       "20    long and boring: i've read this book with much...\n",
       "21    dont like it: this product smells when you ope...\n",
       "24    don't take the chance - get the se branded cab...\n",
       "25    waste of money!: like many of the barbie cd ro...\n",
       "27    has no range: i suppose if you were going to s...\n",
       "29    three days of use and it broke: very disappoin...\n",
       "35    not as expected...: my children get easily bor...\n",
       "37    doublecharged for shipping because merchant wa...\n",
       "39    light reading, light in substance: a clichéd s...\n",
       "41    great book--unacceptable condition: i was look...\n",
       "46    more romance please and less mystery!!: the re...\n",
       "47    a lost author: i have read all of the carpathi...\n",
       "48    okay but messy....: we purchased this bag for ...\n",
       "52    00 y/o potty humor from 00 somethings does not...\n",
       "53    i wish adam sandler wasn't in this.: i should ...\n",
       "56    disappointment at it's extreme: i had never re...\n",
       "61    predictable & has a lame ending: i have read a...\n",
       "63    a historical gem whose title obscures its inte...\n",
       "64    vastly overrated work of fiction: i was sorely...\n",
       "67    had to return it: the sizing is perfect and a ...\n",
       "68    broke the 0nd time i wore it, great until then...\n",
       "72    waste of money: the inner metal rods keep poki...\n",
       "73    causes bruising and blisters if worn too long:...\n",
       "74    not happy: i was really unhappy with this. i f...\n",
       "75    nice product: my wife loves it but we unfortun...\n",
       "77    conspiracy theorist: the author is very likely...\n",
       "78    another disappointment from higgins: this nove...\n",
       "79    0, and thats being generous....: i have read m...\n",
       "83    where does the novel begin?: i found it preach...\n",
       "84    avoid it.: the abridged version of this book w...\n",
       "85    ok...: this book was recommend 0 me from my ne...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_neg.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8888888888888888\n",
      "Recall: 0.7547169811320755\n",
      "F1-Score: 0.8163265306122449\n",
      "Accuracy: 0.82\n",
      "Confusion Matrix:\n",
      " [[42  5]\n",
      " [13 40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "y_true = []  # True labels (0 for negative, 1 for positive)\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "# Evaluate on negative test data\n",
    "for obj in test_neg:\n",
    "    res = classifier.classify(extract_features(obj.split()))\n",
    "    y_true.append(0)  # True label for negative class\n",
    "    y_pred.append(res)  # Predicted label\n",
    "\n",
    "# Evaluate on positive test data\n",
    "for obj in test_pos:\n",
    "    res = classifier.classify(extract_features(obj.split()))\n",
    "    y_true.append(1)  # True label for positive class\n",
    "    y_pred.append(res)  # Predicted label\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "# Print results\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[42  5]\n",
      " [13 40]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7636    0.8936    0.8235        47\n",
      "           1     0.8889    0.7547    0.8163        53\n",
      "\n",
      "    accuracy                         0.8200       100\n",
      "   macro avg     0.8263    0.8242    0.8199       100\n",
      "weighted avg     0.8300    0.8200    0.8197       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\\n\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 y/o potty humor from 00 somethings does not a comedy make....: saw this movie and felt compelled to warn people not to waste 000 agonizing minutes of their lives. in a nutshell, i have heard many more clever jokes and \"one-liners\" watching king of queens on cable.....seriously folks, it's that bad....the bad jokes and the 00 references to small jewish penises got old real quick. thought i would always be a sandler fan but he clearly, adam was in need of some rent money and jumped on this script for the cash and ran!why else would the plot line include seth rogan writing for him??? to put things into perspective, this film made bruno look like casablanca!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_neg.loc[52])\n",
    "classifier.classify(extract_features(test_neg.loc[52].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
