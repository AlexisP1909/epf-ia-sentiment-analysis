{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from layers import SelfAttention\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "\n",
    "import sys\n",
    "# add parent directory to Python path for layers.py access\n",
    "sys.path.append('..')\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet',\n",
    "          'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train_df = pd.read_parquet(\n",
    "    \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"train\"])\n",
    "validation_df = pd.read_parquet(\n",
    "    \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"validation\"])\n",
    "test_df = pd.read_parquet(\n",
    "    \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"test\"])\n",
    "# verse_text, label\n",
    "# label: 0 = negative, 1 = positive, 2 = no_impact, 3 = mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--config'], dest='config', nargs=None, const=None, default=0, type=None, choices=None, required=False, help='Integer value representing a model configuration', metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Argument specification\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\",\n",
    "                    default=0,\n",
    "                    help=\"Integer value representing a model configuration\")\n",
    "# CONFIG OPTIONS:\n",
    "# 0: Simple Multi-Layer Perceptron Model\n",
    "# 1: Simple Multi-Layer Perceptron Model w/ Self-Attention (Non-Penalized)\n",
    "# 2: Simple Multi-Layer Perceptron Model w/ Self-Attention (Penalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "X_train, Y_train, X_validation, Y_validation = train_df['verse_text'].values, train_df['label'].values, validation_df['verse_text'].values, validation_df['label'].values\n",
    "\n",
    "sequence_length = 900\n",
    "vocabulary_size = 10000  # choose 10k most-used words for truncated vocabulary\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_validation = tokenizer.texts_to_sequences(X_validation)\n",
    "\n",
    "X_train = pad_sequences(sequences=X_train, maxlen=sequence_length)\n",
    "X_validation = pad_sequences(sequences=X_validation, maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Attention_Model(config,vocabulary_size, sequence_length, embedding_dims, batch_size, num_epochs, random_seed, X_train, Y_train, X_validation, Y_validation):\n",
    "    tf.random.set_seed(random_seed)\n",
    "    X = Input(shape=(sequence_length,), batch_size=batch_size)\n",
    "\n",
    "    embedded = Embedding(input_dim=vocabulary_size, output_dim=embedding_dims)(X)\n",
    "\n",
    "    # Optional Self-Attention Mechanisms\n",
    "    if config == 1:\n",
    "        embedded, attention_weights = SelfAttention(size=50,\n",
    "                                                    num_hops=6,\n",
    "                                                    use_penalization=False)(embedded)\n",
    "    elif config == 2:\n",
    "        embedded, attention_weights = SelfAttention(size=50,\n",
    "                                                    num_hops=6,\n",
    "                                                    use_penalization=True,\n",
    "                                                    penalty_coefficient=0.1)(embedded)\n",
    "\n",
    "    # Multi-Layer Perceptron\n",
    "    embedded_flattened = Flatten()(embedded)\n",
    "    fully_connected = Dense(units=250, activation='relu')(embedded_flattened)\n",
    "\n",
    "    # Prediction Layer\n",
    "    Y = Dense(units=1, activation='sigmoid')(fully_connected)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=X, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#First Model with 0 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "# choose 1000-word sequences, either pad or truncate sequences to this\n",
    "embedding_dims = 50      # number of dimensions to represent each word in vector space\n",
    "batch_size = 100         # feed in the neural network in 100-example training batches\n",
    "num_epochs = 100\n",
    "config = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = build_Attention_Model(config,vocabulary_size, sequence_length, embedding_dims, batch_size, num_epochs, random_seed, train_df['verse_text'], train_df['label'], validation_df['verse_text'], validation_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.fit(x=X_train, y=Y_train,\n",
    "          validation_data=(X_validation, Y_validation),\n",
    "          epochs=num_epochs, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
