{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import kagglehub\n",
    "from layers import SelfAttention\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense\n",
    "import sys\n",
    "# add parent directory to Python path for layers.py access\n",
    "sys.path.append('..')\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet',\n",
    "\n",
    "\n",
    "\n",
    "#           'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "\n",
    "\n",
    "\n",
    "# train_df = pd.read_parquet(\n",
    "\n",
    "\n",
    "\n",
    "#     \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"train\"])\n",
    "\n",
    "\n",
    "\n",
    "# validation_df = pd.read_parquet(\n",
    "\n",
    "\n",
    "\n",
    "#     \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"validation\"])\n",
    "\n",
    "\n",
    "\n",
    "# test_df = pd.read_parquet(\n",
    "\n",
    "\n",
    "\n",
    "#     \"hf://datasets/google-research-datasets/poem_sentiment/\" + splits[\"test\"])\n",
    "\n",
    "\n",
    "\n",
    "# verse_text, label\n",
    "\n",
    "\n",
    "\n",
    "# label: 0 = negative, 1 = positive, 2 = no_impact, 3 = mixed\n",
    "\n",
    "\n",
    "\n",
    "# Télécharger l'archive\n",
    "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
    "print(\"Chemin de l'archive téléchargée :\", path)\n",
    "\n",
    "# Dossier de destination pour déplacer l'archive\n",
    "destination_dir = \"C:/Users/alexi/aProjets_Code/IA Projet 2/epf-ia-sentiment-analysis/amazonreviews_dataset\"\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Chemin complet de l'archive dans le dossier de destination\n",
    "# Récupère le nom de l'archive (ex: \"archive.zip\")\n",
    "archive_name = os.path.basename(path)\n",
    "new_archive_path = os.path.join(destination_dir, archive_name)\n",
    "\n",
    "# Déplacer l'archive vers le dossier de destination\n",
    "shutil.move(path, new_archive_path)\n",
    "print(f\"Archive déplacée vers : {new_archive_path}\")\n",
    "\n",
    "# Dossier pour extraire l'archive\n",
    "extract_dir = os.path.join(destination_dir, \"amazonreviews\")\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "print(f\"Archive extraite dans : {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Training data\n",
    "X_train = train_df['verse_text'].values\n",
    "Y_train = train_df['label'].values\n",
    "\n",
    "# Validation data\n",
    "X_validation = validation_df['verse_text'].values\n",
    "Y_validation = validation_df['label'].values\n",
    "\n",
    "# Test data\n",
    "X_test = test_df['verse_text'].values\n",
    "Y_test = test_df['label'].values\n",
    "\n",
    "# Y_train = to_categorical(Y_train, num_classes=4)\n",
    "# Y_validation = to_categorical(Y_validation, num_classes=4)\n",
    "# Y_test = to_categorical(Y_test, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000  # choose 10k most-used words for truncated vocabulary\n",
    "sequence_length = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_validation = tokenizer.texts_to_sequences(X_validation)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#One hot encoding\n",
    "X_train = pad_sequences(sequences=X_train, maxlen=sequence_length,padding='post', truncating='post')\n",
    "X_validation = pad_sequences(sequences=X_validation, maxlen=sequence_length,padding='post', truncating='post')\n",
    "X_test = pad_sequences(sequences=X_test, maxlen=sequence_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Attention_Model(config,vocabulary_size, sequence_length, embedding_dims, batch_size, num_epochs, random_seed, X_train, Y_train, X_validation, Y_validation):\n",
    "    tf.random.set_seed(random_seed)\n",
    "    X = Input(shape=(sequence_length,), batch_size=batch_size)\n",
    "\n",
    "    embedded = Embedding(input_dim=vocabulary_size, output_dim=embedding_dims)(X)\n",
    "\n",
    "    # Optional Self-Attention Mechanisms\n",
    "    if config == 1:\n",
    "        embedded, attention_weights = SelfAttention(size=50,\n",
    "                                                    num_hops=6,\n",
    "                                                    use_penalization=False)(embedded)\n",
    "    elif config == 2:\n",
    "        embedded, attention_weights = SelfAttention(size=50,\n",
    "                                                    num_hops=6,\n",
    "                                                    use_penalization=True,\n",
    "                                                    penalty_coefficient=0.1)(embedded)\n",
    "\n",
    "    # Multi-Layer Perceptron\n",
    "    embedded_flattened = Flatten()(embedded)\n",
    "    fully_connected = Dense(units=250, activation='relu')(embedded_flattened)\n",
    "\n",
    "    # Prediction Layer\n",
    "    Y = Dense(units=1, activation='softmax')(fully_connected)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=X, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#First Model with 0 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG OPTIONS:\n",
    "# 0: Simple Multi-Layer Perceptron Model\n",
    "# 1: Simple Multi-Layer Perceptron Model w/ Self-Attention (Non-Penalized)\n",
    "# 2: Simple Multi-Layer Perceptron Model w/ Self-Attention (Penalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "# choose 1000-word sequences, either pad or truncate sequences to this\n",
    "embedding_dims = 50      # number of dimensions to represent each word in vector space\n",
    "batch_size = 100         # feed in the neural network in 100-example training batches\n",
    "num_epochs = 100\n",
    "config = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = build_Attention_Model(config,vocabulary_size, sequence_length, embedding_dims, batch_size, num_epochs, random_seed, train_df['verse_text'], train_df['label'], validation_df['verse_text'], validation_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.fit(x=X_train, y=Y_train,\n",
    "          validation_data=(X_validation, Y_validation),\n",
    "          epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = perceptron.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Get the predicted class (index of the highest probability)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_test_classes = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred[:-1])\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(perceptron, X_validation, Y_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
